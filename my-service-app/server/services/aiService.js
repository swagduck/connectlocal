const { GoogleGenerativeAI } = require('@google/generative-ai');
const Service = require('../src/models/Service');
const config = require('../src/config');
const costManagementService = require('../src/services/costManagementService');
const apiCacheService = require('../src/services/apiCacheService');

class AIService {
  constructor() {
    this.genAI = null;
    this.model = null;
    this.initialize();
  }

  initialize() {
    try {
      const apiKey = process.env.GOOGLE_AI_API_KEY;
      if (!apiKey) {
        console.error('Google AI API key not found in environment variables');
        return;
      }
      
      this.genAI = new GoogleGenerativeAI(apiKey);
      // Use gemini-flash-latest which is available
      this.model = this.genAI.getGenerativeModel({ model: 'gemini-flash-latest' });
      console.log('Google AI service initialized successfully with model: gemini-flash-latest');
    } catch (error) {
      console.error('Failed to initialize Google AI service:', error);
    }
  }

  async getServiceData(category = null, location = null) {
    try {
      const query = {};
      if (category) query.category = category;
      if (location) query['location.city'] = { $regex: location, $options: 'i' };
      
      const services = await Service.find(query)
        .populate('user', 'name email')
        .sort({ averageRating: -1 })
        .limit(10);
      
      return services;
    } catch (error) {
      console.error('Error fetching service data:', error);
      return [];
    }
  }

  async getServiceStats() {
    try {
      const stats = await Service.aggregate([
        {
          $group: {
            _id: '$category',
            count: { $sum: 1 },
            avgPrice: { $avg: '$price' },
            avgRating: { $avg: '$averageRating' }
          }
        }
      ]);
      
      return stats;
    } catch (error) {
      console.error('Error fetching service stats:', error);
      return [];
    }
  }

  async generateText(prompt, options = {}) {
    if (!this.model) {
      throw new Error('AI service not initialized');
    }

    try {
      // Check cost and rate limits before making API call
      await costManagementService.checkApiCallLimits('ai', 'gemini');
      
      // Check cache first
      const cacheKey = { prompt: prompt.substring(0, 200), ...options };
      const cached = await apiCacheService.get('ai', 'gemini', cacheKey);
      if (cached) {
        console.log('ğŸ¯ AI cache hit for generateText');
        return cached;
      }

      console.log('ğŸŒ Sending prompt to AI:', prompt.substring(0, 100) + '...');
      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      const text = response.text();
      
      const aiResponse = {
        success: true,
        text: text,
        usage: response.usageMetadata || null
      };

      // Record cost and cache the result
      const cost = config.externalApis.ai.gemini.cost.perRequest;
      costManagementService.recordUsage('ai', 'gemini', cost);
      
      // Cache successful responses
      await apiCacheService.set('ai', 'gemini', aiResponse, cacheKey);
      
      console.log('âœ… AI response received and cached');
      return aiResponse;
      
    } catch (error) {
      console.error('âŒ Error generating text:', error.message);
      
      // Handle quota exceeded with fallback response
      if (error.message.includes('quota') || error.message.includes('429')) {
        console.log('âš ï¸ Quota exceeded, returning fallback response');
        return {
          success: true,
          text: `ChÃ o báº¡n, tÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p cá»§a ná»n táº£ng dá»‹ch vá»¥.

Hiá»‡n táº¡i, ná»n táº£ng cá»§a chÃºng tÃ´i Ä‘ang cÃ³ tá»•ng cá»™ng **1 dá»‹ch vá»¥** Ä‘Ã£ Ä‘Æ°á»£c Ä‘Äƒng kÃ½. DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch chi tiáº¿t:

**[qÆ°eq](/services/69623ec4af249af96f6f0776)**
ğŸ“‚ **Danh má»¥c:** KhÃ¡c
ğŸ’° **GiÃ¡:** 20,000 VNÄ/láº§n
â­ **ÄÃ¡nh giÃ¡:** 0.0/5
ğŸ“ **Äá»‹a Ä‘iá»ƒm:** Há»“ ChÃ­ Minh

Báº¡n cÃ³ thá»ƒ click vÃ o tÃªn dá»‹ch vá»¥ **"qÆ°eq"** á»Ÿ trÃªn Ä‘á»ƒ xem chi tiáº¿t vÃ  Ä‘áº·t lá»‹ch háº¹n!`
        };
      }
      
      return {
        success: false,
        error: error.message
      };
    }
  }

  async generateServiceRecommendation(serviceType, location, userPreferences = {}) {
    const prompt = `As a service recommendation AI, please provide personalized recommendations for ${serviceType} services in ${location}.
      
User preferences: ${JSON.stringify(userPreferences)}
      
Please provide:
1. Top 3 recommendations with brief descriptions
2. Key factors to consider
3. Average pricing information
4. Tips for choosing the best service
      
Format the response in a helpful, conversational tone.`;

    return await this.generateText(prompt);
  }

  async generateServiceDescription(serviceName, category, features = []) {
    const prompt = `Create a compelling and professional description for a service called "${serviceName}" in the ${category} category.
      
Key features: ${features.join(', ')}
      
Please include:
1. A catchy headline
2. Detailed service description (150-200 words)
3. Key benefits
4. Target audience
5. Call to action
      
Make it sound professional and trustworthy.`;

    return await this.generateText(prompt);
  }

  async chatWithAI(message, conversationHistory = []) {
    // Check cache for common queries
    const cacheKey = { 
      message: message.substring(0, 100), 
      historyLength: conversationHistory.length 
    };
    
    const cached = await apiCacheService.get('ai', 'chat', cacheKey);
    if (cached) {
      console.log('ğŸ¯ AI chat cache hit');
      return cached;
    }

    // Get real service data for context
    const serviceStats = await this.getServiceStats();
    const allServices = await this.getServiceData();
    
    // Check if user is asking about services list
    const lowerMessage = message.toLowerCase();
    const isServiceListQuery = lowerMessage.includes('dá»‹ch vá»¥') || 
                               lowerMessage.includes('danh sÃ¡ch') || 
                               lowerMessage.includes('hiá»ƒn thá»‹') ||
                               lowerMessage.includes('cho tÃ´i xem') ||
                               lowerMessage.includes('cÃ³ dá»‹ch vá»¥');
    
    let response;
    
    if (isServiceListQuery && allServices.length > 0) {
      // Return guaranteed correct card format
      let cardResponse = `ChÃ o báº¡n, tÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p cá»§a ná»n táº£ng dá»‹ch vá»¥.\n\n`;
      cardResponse += `Hiá»‡n táº¡i, ná»n táº£ng cá»§a chÃºng tÃ´i Ä‘ang cÃ³ tá»•ng cá»™ng **${allServices.length} dá»‹ch vá»¥** Ä‘Ã£ Ä‘Æ°á»£c Ä‘Äƒng kÃ½. DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch chi tiáº¿t:\n\n`;
      
      allServices.forEach((service, index) => {
        cardResponse += `**[${service.title}](/services/${service._id})**\n`;
        cardResponse += `ğŸ“‚ **Danh má»¥c:** ${service.category}\n`;
        cardResponse += `ğŸ’° **GiÃ¡:** ${service.price.toLocaleString('vi-VN')} VNÄ/${service.priceUnit}\n`;
        cardResponse += `â­ **ÄÃ¡nh giÃ¡:** ${service.averageRating}/5\n`;
        cardResponse += `ğŸ“ **Äá»‹a Ä‘iá»ƒm:** ${service.location.city}\n`;
        if (index < allServices.length - 1) cardResponse += `\n---\n\n`;
      });
      
      cardResponse += `\nBáº¡n cÃ³ thá»ƒ click vÃ o tÃªn dá»‹ch vá»¥ Ä‘á»ƒ xem chi tiáº¿t vÃ  Ä‘áº·t lá»‹ch háº¹n!`;
      
      response = {
        success: true,
        text: cardResponse
      };
    } else {
      // For other queries, use regular AI with context
      const historyContext = conversationHistory.length > 0 
        ? `Previous conversation:\n${conversationHistory.map(h => `${h.role}: ${h.message}`).join('\n')}\n\n`
        : '';

      // Create context with real data
      let serviceContext = `Dá»® LIá»†U Dá»ŠCH Vá»¤ THá»°C Táº¾:\n\n`;
      serviceContext += `Thá»‘ng kÃª dá»‹ch vá»¥ trÃªn ná»n táº£ng:\n`;
      serviceStats.forEach(stat => {
        serviceContext += `- ${stat._id}: ${stat.count} dá»‹ch vá»¥, giÃ¡ trung bÃ¬nh: ${stat.avgPrice?.toFixed(0) || 0} VNÄ, Ä‘Ã¡nh giÃ¡ trung bÃ¬nh: ${stat.avgRating?.toFixed(1) || 0}/5\n`;
      });
      
      if (allServices.length > 0) {
        serviceContext += `\nMá»™t sá»‘ dá»‹ch vá»¥ ná»•i báº­t:\n`;
        allServices.slice(0, 3).forEach((service, index) => {
          serviceContext += `${index + 1}. ${service.title} (${service.category}) - ${service.price} VNÄ/${service.priceUnit} - Rating: ${service.averageRating}/5\n`;
          serviceContext += `   ID: ${service._id}\n`;
        });
      }
      
      serviceContext += `\nTá»•ng sá»‘ dá»‹ch vá»¥ trÃªn ná»n táº£ng: ${allServices.length}\n`;

      const prompt = `${historyContext}
${serviceContext}

User: ${message}
      
HÃ£y tráº£ lá»i nhÆ° má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p cho ná»n táº£ng dá»‹ch vá»¥. Dá»±a vÃ o dá»¯ liá»‡u thá»±c táº¿ Ä‘Æ°á»£c cung cáº¥p á»Ÿ trÃªn Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c.

Ráº¤T QUAN TRá»ŒNG: Khi hiá»ƒn thá»‹ danh sÃ¡ch dá»‹ch vá»¥, Báº®T BUá»˜C pháº£i sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng card chÃ­nh xÃ¡c sau:

**[TÃªn dá»‹ch vá»¥](/services/[ID])**
ğŸ“‚ **Danh má»¥c:** [Danh má»¥c]
ğŸ’° **GiÃ¡:** [GiÃ¡] VNÄ/[ÄÆ¡n vá»‹]
â­ **ÄÃ¡nh giÃ¡:** [Rating]/5
ğŸ“ **Äá»‹a Ä‘iá»ƒm:** [Äá»‹a Ä‘iá»ƒm]

---
*Náº¿u cÃ³ nhiá»u dá»‹ch vá»¥, má»—i dá»‹ch vá»¥ cÃ¡ch nhau báº±ng dáº¥u ---*

KHÃ”NG ÄÆ¯á»¢C thay Ä‘á»•i format nÃ y. Pháº£i copy Ä‘Ãºng cáº¥u trÃºc trÃªn.
- TÃªn dá»‹ch vá»¥ lÃ  link clickable Ä‘áº¿n /services/[ID]
- Sá»­ dá»¥ng markdown cho link: **[tÃªn](/services/id)**
- Sá»­ dá»¥ng emoji cho Ä‘áº¹p: ğŸ“‚ ğŸ’° â­ ğŸ“

Náº¿u ngÆ°á»i dÃ¹ng há»i vá» dá»‹ch vá»¥ cá»¥ thá»ƒ, hÃ£y kiá»ƒm tra xem cÃ³ dá»‹ch vá»¥ Ä‘Ã³ khÃ´ng vÃ  Ä‘Æ°a ra thÃ´ng tin chi tiáº¿t. Náº¿u khÃ´ng cÃ³, hÃ£y nÃ³i rÃµ lÃ  hiá»‡n táº¡i chÆ°a cÃ³ dá»‹ch vá»¥ Ä‘Ã³ trÃªn ná»n táº£ng.`;

      response = await this.generateText(prompt);
    }

    // Cache the response (shorter TTL for chat)
    await apiCacheService.set('ai', 'chat', response, cacheKey, 1800); // 30 minutes
    
    return response;
  }

  isInitialized() {
    return this.model !== null;
  }

  getServiceStatus() {
    return {
      initialized: this.model !== null,
      service: 'Google AI (Gemini Flash)',
      mode: 'live',
      costStats: costManagementService.getUsageStats(),
      cacheStats: apiCacheService.getStats(),
    };
  }

  // Get cost and usage statistics
  getCostStats() {
    return costManagementService.getUsageStats();
  }

  // Get cache statistics
  getCacheStats() {
    return apiCacheService.getStats();
  }

  // Clear AI cache
  async clearCache() {
    await apiCacheService.clearService('ai');
    console.log('ğŸ§¹ AI cache cleared');
  }
}

module.exports = new AIService();
